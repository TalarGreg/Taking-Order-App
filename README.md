# MS Fabric Taking Order App (toa)

## General description
The solution is built natively on Microsoft Fabric and implements an end-to-end data pipeline for real-time order ingestion, transformation, forecasting, and reporting.

Daily Sales Dashboard – aggregated restaurant sales metrics refreshed from the gold layer.

10-Minute Forecasting Analysis – exploratory notebooks that generate predictive insights on near-term demand, based on machine learning models trained and evaluated in Fabric.

Synthetic (randomized) order events are generated by a local Python script and ingested through Eventstream into the Lakehouse bronze tier. Data Pipelines orchestrate PySpark notebooks that process and partition silver tables (orders, items, weather). From there, gold tables provide aggregated daily sales and features for machine learning. 

Training notebooks train and evaluate multiple models with metrics (RMSE, MAE, sMAPE, Accuracy@10%) tracked in MLflow.

## Project Architecture
<img width="1409" height="883" alt="image" src="https://github.com/user-attachments/assets/d4693569-f86a-4a05-93ee-4214397df11d" />

# 1. Python script - data generation

The `gen_toa_data_v2.py script` generates JSON order events and sends them to the Eventstream `es_toa`.
It leverages the azure-eventhub Python SDK to publish data into Microsoft Fabric Eventstream.
<img width="1153" height="134" alt="image" src="https://github.com/user-attachments/assets/3aaea121-d9da-40f4-87fb-13baf1601a44" />
<img width="1598" height="643" alt="image" src="https://github.com/user-attachments/assets/a251e1c9-1f31-454c-9dfa-9761dfe58788" />


The script uses a small file `order_id_counter.txt` to persist the current order ID so that, if data generation is interrupted, the sequence can continue seamlessly on the next run.

To run the script correctly, it is essential to provide valid `EVENTHUB_NAME` and `CONNECTION_STR` environment variables. These values can be obtained directly from the Eventstream Live mode under Source details.
<img width="1476" height="719" alt="es_details" src="https://github.com/user-attachments/assets/e7fd2ccd-4f57-42bb-a18c-19700ff1716c" />

# 2. Eventstream – ingest and data distribution
As shown in the screenshots, 'es_toa' captures the incoming data and loads it into the Bronze tier of the Lakehouse 'lh_toa' as the table 'tbl_bronze_raw'.
In parallel, the same stream is ingested into the Eventhouse 'eh_toa' and stored in the KQL database as the table 'eventstream_table'.
<img width="1348" height="354" alt="image" src="https://github.com/user-attachments/assets/a9242de4-1e89-4e90-addc-b8cf22d15f10" />

# 3. Eventhouse and Real-Time Report eh_rtdb_last_10m_orders
Data lands in Eventhouse (KQL Database) 'eh_toa' table 'eventstream_table':
<img width="1515" height="775" alt="image" src="https://github.com/user-attachments/assets/9441f655-f0b9-4feb-997f-a7a5673702a8" />

Real-Time Report base on KQL table using following query which shows agregated:

<img width="427" height="200" alt="image" src="https://github.com/user-attachments/assets/ba15d6ee-b7a5-40bc-a88e-99e1aca51941" />

This KQL query retrieves data from the 'eventstream_table' for the last 10 minutes. Since the items column contains nested JSON data, the query first converts it into a dynamic object and then expands it into individual keys (representing product names). For each product, it extracts its name (item) and quantity (qty). The quantities are then aggregated to calculate the total number of units sold per product within the time window. The result is a list of products with their total sales, ordered in descending order by product name.

<img width="565" height="407" alt="image" src="https://github.com/user-attachments/assets/002474a2-98e1-4043-8fb8-316c5c21e8f3" />

# 4. Data orchestration – Data pipelines pl_etl_silver - step 1
The data pipeline triggers the `notebooks/nb_etl_silver` notebook every 10 minutes. This ensures the Silver layer is refreshed at a reasonably high frequency, which is sufficient for reporting purposes.
<img width="1725" height="521" alt="image" src="https://github.com/user-attachments/assets/c3baed96-41fe-4279-9776-3fee065eec28" />

The notebook uses a Change Data Capture (CDC) mechanism, with the control table `cdc_control` storing the last processed Delta version. If no changes are detected in `tbl_bronze_raw`, the script terminates with the message “No new changes to process.” When new data appears, it selects the insert operations since the last refresh, applies basic data quality rules `total_amount >= 0`, and normalizes key fields:
<img width="574" height="665" alt="image" src="https://github.com/user-attachments/assets/cf2399e9-16af-4412-8b51-cfc4e151a466" />


Parses and casts identifiers `order_id, restaurant_id, register_id, employee_id` and categorical enums `register_type, order_size` into integer surrogate keys.

Unnests nested structures:

`items` (a struct derived from JSON) → produces `items_df`, expanding each product into a `<product>_qty` column, while retaining `order_id` and `partition_date`.

`IsWeather` - extracts weather attributes into `weather_df`.

The process generates three curated Silver tables: `stg_silver_orders, stg_silver_items, and stg_silver_weather`.

Each write is tagged with a `batch_id` equal to the current Delta version for full traceability. Tables are written sequentially, and a transaction-like guard tracks successful writes. If a failure occurs, the job performs a selective rollback `DELETE … WHERE batch_id = <id>` only on the tables already saved, ensuring consistency without requiring full rewrites. On success, the control table is updated to the `current_version`.

Additionally, to improve performance, the tables 'stg_silver_orders, stg_silver_items, and stg_silver_weather' are partitioned by partition_date.
<img width="1095" height="509" alt="image" src="https://github.com/user-attachments/assets/104cf2de-2af8-4088-9982-cb99f5b3a0be" />




















