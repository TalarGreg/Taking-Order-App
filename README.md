# MS Fabric Taking Order App (toa)

## Table of Contents
- [1. Python script - data generation](#1-python-script---data-generation)
- [2. Eventstream – ingest and data distribution](#2-eventstream--ingest-and-data-distribution)

## General description
The solution is built natively on Microsoft Fabric and implements an end-to-end data pipeline for real-time order ingestion, transformation, forecasting, and reporting.

Daily Sales Dashboard – aggregated restaurant sales metrics refreshed from the gold layer.

10-Minute Forecasting Analysis – exploratory notebooks that generate predictive insights on near-term demand, based on machine learning models trained and evaluated in Fabric.

Synthetic (randomized) order events are generated by a local Python script and ingested through Eventstream into the Lakehouse bronze tier. Data Pipelines orchestrate PySpark notebooks that process and partition silver tables (orders, items, weather). From there, gold tables provide aggregated daily sales and features for machine learning. 

Training notebooks train and evaluate multiple models with metrics (RMSE, MAE, sMAPE, Accuracy@10%) tracked in MLflow.

## Project Architecture
### MS Fabric lineage view
<img width="1409" height="883" alt="image" src="https://github.com/user-attachments/assets/d4693569-f86a-4a05-93ee-4214397df11d" />

### Data Flow Diagram
<img width="1990" height="1202" alt="toa-Data Flow Diagram drawio" src="https://github.com/user-attachments/assets/2ffd0967-1262-41ac-bc8b-58fe9bd9d20a" />


# 1. Python script - data generation

The `gen_toa_data_v2.py script` generates JSON order events and sends them to the Eventstream `es_toa`.
It leverages the azure-eventhub Python SDK to publish data into Microsoft Fabric Eventstream.
<img width="1153" height="134" alt="image" src="https://github.com/user-attachments/assets/3aaea121-d9da-40f4-87fb-13baf1601a44" />
<img width="1598" height="643" alt="image" src="https://github.com/user-attachments/assets/a251e1c9-1f31-454c-9dfa-9761dfe58788" />


The script uses a small file `order_id_counter.txt` to persist the current order ID so that, if data generation is interrupted, the sequence can continue seamlessly on the next run.

To run the script correctly, it is essential to provide valid `EVENTHUB_NAME` and `CONNECTION_STR` environment variables. These values can be obtained directly from the Eventstream Live mode under Source details.
<img width="1476" height="719" alt="es_details" src="https://github.com/user-attachments/assets/e7fd2ccd-4f57-42bb-a18c-19700ff1716c" />

# 2. Eventstream – ingest and data distribution
As shown in the screenshots, 'es_toa' captures the incoming data and loads it into the Bronze tier of the Lakehouse 'lh_toa' as the table 'tbl_bronze_raw'.
In parallel, the same stream is ingested into the Eventhouse 'eh_toa' and stored in the KQL database as the table 'eventstream_table'.
<img width="1348" height="354" alt="image" src="https://github.com/user-attachments/assets/a9242de4-1e89-4e90-addc-b8cf22d15f10" />

# 3. Eventhouse and Real-Time Report eh_rtdb_last_10m_orders
Data lands in Eventhouse (KQL Database) 'eh_toa' table 'eventstream_table':
<img width="1515" height="775" alt="image" src="https://github.com/user-attachments/assets/9441f655-f0b9-4feb-997f-a7a5673702a8" />

Real-Time Report base on KQL table using following query which shows agregated:

<img width="427" height="200" alt="image" src="https://github.com/user-attachments/assets/ba15d6ee-b7a5-40bc-a88e-99e1aca51941" />

This KQL query retrieves data from the 'eventstream_table' for the last 10 minutes. Since the items column contains nested JSON data, the query first converts it into a dynamic object and then expands it into individual keys (representing product names). For each product, it extracts its name (item) and quantity (qty). The quantities are then aggregated to calculate the total number of units sold per product within the time window. The result is a list of products with their total sales, ordered in descending order by product name.

<img width="565" height="407" alt="image" src="https://github.com/user-attachments/assets/002474a2-98e1-4043-8fb8-316c5c21e8f3" />

# 4. Data orchestration – Data pipelines pl_etl_silver - step 1
The data pipeline triggers the `notebooks/nb_etl_silver` notebook every 10 minutes. This ensures the Silver layer is refreshed at a reasonably high frequency, which is sufficient for reporting purposes.
<img width="1725" height="521" alt="image" src="https://github.com/user-attachments/assets/c3baed96-41fe-4279-9776-3fee065eec28" />

The notebook uses a Change Data Capture (CDC) mechanism, with the control table `cdc_control` storing the last processed Delta version. If no changes are detected in `tbl_bronze_raw`, the script terminates with the message “No new changes to process.” When new data appears, it selects the insert operations since the last refresh, applies basic data quality rules `total_amount >= 0`, and normalizes key fields:
<img width="574" height="665" alt="image" src="https://github.com/user-attachments/assets/cf2399e9-16af-4412-8b51-cfc4e151a466" />


Parses and casts identifiers `order_id, restaurant_id, register_id, employee_id` and categorical enums `register_type, order_size` into integer surrogate keys.

Unnests nested structures:

`items` (a struct derived from JSON) → produces `items_df`, expanding each product into a `<product>_qty` column, while retaining `order_id` and `partition_date`.

`IsWeather` - extracts weather attributes into `weather_df`.

The process generates three curated Silver tables: `stg_silver_orders, stg_silver_items, and stg_silver_weather`.

Each write is tagged with a `batch_id` equal to the current Delta version for full traceability. Tables are written sequentially, and a transaction-like guard tracks successful writes. If a failure occurs, the job performs a selective rollback `DELETE … WHERE batch_id = <id>` only on the tables already saved, ensuring consistency without requiring full rewrites. On success, the control table is updated to the `current_version`.

Additionally, to improve performance, the tables 'stg_silver_orders, stg_silver_items, and stg_silver_weather' are partitioned by partition_date.
<img width="1095" height="509" alt="image" src="https://github.com/user-attachments/assets/104cf2de-2af8-4088-9982-cb99f5b3a0be" />


# 5. Data orchestration – Data pipelines pl_etl_gold_agg_daily_sales step 2
Daily sales upsert (Gold layer) with 7-day rolling refresh for performance and data hygiene.
This query aggregates daily sales per restaurant from the Silver table `stg_silver_orders` and MERGEs the results into the Gold table `tbl_gold_agg_daily_sales`. The MERGE pattern performs an upsert: it updates existing rows and inserts new ones based on the composite key `partition_date, restaurant_id`. To keep the job efficient and focused on data that is still changing, the pipeline recomputes only the last 7 days:

The source window is restricted with `WHERE partition_date >= current_date() - 7`, so only the most recent 7 days are recalculated. Late-arriving data older than 7 days is intentionally ignored and will not be loaded into the Gold layer. Using MERGE avoids full overwrites, reduces compute and I/O, and preserves partition pruning on partition_date.
`sales_amount` is cast to `DECIMAL(18,2` to ensure consistent currency precision.

<img width="550" height="378" alt="image" src="https://github.com/user-attachments/assets/c6dabd91-cb9f-4d53-a44e-5681cf18a044" />

tbl_gold_agg_daily_sales remains an up-to-date, query-friendly fact table for dashboards, refreshed quickly by targeting only the active 7-day window.


# 6. Report pb_gold_agg_daily_sales
This is a simple but effective analytical report built on top of the Gold table `tbl_gold_agg_daily_sales`. The report presents the daily total sales amount and visualizes how this metric changes over time. Its main purpose is to provide a clear trend view of overall restaurant sales, supporting quick insight into growth patterns, seasonality, or sudden drops in revenue.
<img width="1454" height="865" alt="image" src="https://github.com/user-attachments/assets/6e57644d-7056-47a7-9285-1b059e245c71" />


# 7. ML Model

The goal of this ML model is to provide predictive insights for restaurant staff, allowing them to easily estimate the expected number of orders in the next 10 minutes. Such forecasts help both the kitchen and front-of-house teams plan resources more effectively based on anticipated demand.

Input data comes from the Silver tables `stg_silver_orders, stg_silver_items`, and `stg_silver_weather`. 
These datasets are combined into a single DataFrame, followed by a series of transformations and aggregations:

  - weather conditions are converted from categorical values (rainy, snowy, sunny, cloudy) into numeric codes (1–4).

  - null values are filtered out to maintain data quality.
  
<img width="526" height="345" alt="image" src="https://github.com/user-attachments/assets/51a934be-1f6d-4f51-8c64-6ef3b2842f3a" />

  Data is aggregated into daily 10-minute buckets, including:

  - time features: bucket_time (HH:mm), day_of_week.

  - weather features: temperature_avg, humidity_avg, condition_code.

  - sales features: product quantities, lags, rolling means, and day-over-day changes.

The label for the model is `qty_next_10m`, representing the number of items expected in the next bucket. The training dataset therefore contains both `qty_10m` (total sales in the current 10-minute window) and the shifted label for the following bucket.
<img width="542" height="389" alt="image" src="https://github.com/user-attachments/assets/104b05d5-9454-4efe-b3ef-b0a8a6d83c6b" />

<img width="680" height="553" alt="image" src="https://github.com/user-attachments/assets/5579f8fe-59c1-4450-b67d-7a1939003944" />

I transformed the raw labeled data into a fully numeric feature set suitable for machine learning. Time features were encoded with sin/cos transformations (to capture cyclic patterns of hours and weekdays), weather attributes were cast to numeric values with additional binary flags (e.g., is_rain), and sales dynamics were enriched with lags, rolling averages, and change deltas. Missing values were filled with defaults to avoid nulls, and the target label `qty_next_10m` was retained. The output is a clean numeric dataset `df_train_num` ready for training regression models.

<img width="1435" height="278" alt="image" src="https://github.com/user-attachments/assets/a6c7d084-cf37-4207-a960-edaf9c7965f4" />

Model training was conducted in Microsoft Fabric using notebook `nb_ml_model_training_set` and the experiment `eh_10m_forecast_simple`. Two candidate models were evaluated: Linear Regression (LR) and Gradient Boosted Trees (GBT), both trained on one week of historical data. The results were assessed using standard regression metrics (RMSE and MAE), which guided the selection of the preferred model.

<img width="967" height="563" alt="image" src="https://github.com/user-attachments/assets/55bba4d9-96d5-483b-a7b1-9f046292a498" />

<img width="962" height="554" alt="image" src="https://github.com/user-attachments/assets/a12cab13-1c79-4eac-8189-183442356bcf" />

Two candidate models were evaluated for the 10-minute order forecasting task: Gradient Boosted Trees (GBT) and Linear Regression (LR). The performance was measured using MAE (Mean Absolute Error) and RMSE (Root Mean Squared Error). The results showed that LR outperformed GBT on both metrics — achieving an MAE of 43.17 compared to 50.44 for GBT, and an RMSE of 57.64 compared to 117.25. These outcomes indicate that the linear model provided more stable and accurate predictions, while the GBT model likely overfit given the relatively small dataset and the mostly linear nature of the features (time buckets, lags, weather).


