# MS Fabric Taking Order App (toa)

## General description
The solution is built natively on Microsoft Fabric and implements an end-to-end data pipeline for real-time order ingestion, transformation, forecasting, and reporting.

Daily Sales Dashboard – aggregated restaurant sales metrics refreshed from the gold layer.

10-Minute Forecasting Analysis – exploratory notebooks that generate predictive insights on near-term demand, based on machine learning models trained and evaluated in Fabric.

Synthetic (randomized) order events are generated by a local Python script and ingested through Eventstream into the Lakehouse bronze tier. Data Pipelines orchestrate PySpark notebooks that process and partition silver tables (orders, items, weather). From there, gold tables provide aggregated daily sales and features for machine learning. 

Training notebooks train and evaluate multiple models with metrics (RMSE, MAE, sMAPE, Accuracy@10%) tracked in MLflow.

## Project Architecture
<img width="1409" height="883" alt="image" src="https://github.com/user-attachments/assets/d4693569-f86a-4a05-93ee-4214397df11d" />

# 1. Python script - data generation

The `gen_toa_data_v2.py script` generates JSON order events and sends them to the Eventstream `es_toa`.
It leverages the azure-eventhub Python SDK to publish data into Microsoft Fabric Eventstream.
<img width="1153" height="134" alt="image" src="https://github.com/user-attachments/assets/3aaea121-d9da-40f4-87fb-13baf1601a44" />
<img width="1598" height="643" alt="image" src="https://github.com/user-attachments/assets/a251e1c9-1f31-454c-9dfa-9761dfe58788" />


The script uses a small file `order_id_counter.txt` to persist the current order ID so that, if data generation is interrupted, the sequence can continue seamlessly on the next run.

To run the script correctly, it is essential to provide valid `EVENTHUB_NAME` and `CONNECTION_STR` environment variables. These values can be obtained directly from the Eventstream Live mode under Source details.
<img width="1476" height="719" alt="es_details" src="https://github.com/user-attachments/assets/e7fd2ccd-4f57-42bb-a18c-19700ff1716c" />

# 2. Eventstream – ingest and data distribution
As shown in the screenshots, 'es_toa' captures the incoming data and loads it into the Bronze tier of the Lakehouse 'lh_toa' as the table 'tbl_bronze_raw'.
In parallel, the same stream is ingested into the Eventhouse 'eh_toa' and stored in the KQL database as the table 'eventstream_table'.
<img width="1348" height="354" alt="image" src="https://github.com/user-attachments/assets/a9242de4-1e89-4e90-addc-b8cf22d15f10" />

# 3. Eventhouse and Real-Time Report eh_rtdb_last_10m_orders
Data lands in Eventhouse (KQL Database) 'eh_toa' table 'eventstream_table':
<img width="1515" height="775" alt="image" src="https://github.com/user-attachments/assets/9441f655-f0b9-4feb-997f-a7a5673702a8" />

Real-Time Report base on KQL table using following query which shows agregated:

<img width="427" height="200" alt="image" src="https://github.com/user-attachments/assets/ba15d6ee-b7a5-40bc-a88e-99e1aca51941" />

This KQL query retrieves data from the 'eventstream_table' for the last 10 minutes. Since the items column contains nested JSON data, the query first converts it into a dynamic object and then expands it into individual keys (representing product names). For each product, it extracts its name (item) and quantity (qty). The quantities are then aggregated to calculate the total number of units sold per product within the time window. The result is a list of products with their total sales, ordered in descending order by product name.

<img width="565" height="407" alt="image" src="https://github.com/user-attachments/assets/002474a2-98e1-4043-8fb8-316c5c21e8f3" />



























