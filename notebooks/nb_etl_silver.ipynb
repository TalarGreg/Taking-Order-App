{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import to_date, col, lit\n","\n","\n","control_df = spark.table(\"cdc_control\") \\\n","                  .filter(\"name = 'nb_etl_silver'\")\n","last_version = control_df.collect()[0][\"last_processed_version\"]\n","\n","current_version = spark.sql(\"DESCRIBE HISTORY tbl_bronze_raw\")\\\n","                       .selectExpr(\"max(version) as v\")\\\n","                       .first()[\"v\"]\n","\n","if current_version > last_version:\n","\n","    changes_df = spark.sql(f\"\"\"\n","    select  \n","        TRY_CAST(SUBSTRING(order_id, 7) AS INT) as order_id, \n","        CAST(TIMESTAMP as DATE) as partition_date,\n","        timestamp,\n","        TRY_CAST(SUBSTRING(restaurant_id, 12) AS INT) as restaurant_id, \n","        TRY_CAST((CASE register_type\n","            WHEN 'drive'      THEN 1\n","            WHEN 'kiosk'      THEN 2\n","            WHEN 'stationary' THEN 3\n","            ELSE 0\n","        END) AS INT) AS register_type_id,\n","        CAST(substring_index(register_id, '_', -1) AS INT) as register_id,\n","        CAST(substring_index(employee_id, '_', -1) AS INT) AS employee_id,\n","        TRY_CAST((CASE order_size\n","            WHEN 'small'      THEN 1\n","            WHEN 'large'      THEN 2\n","            ELSE 0\n","        END) AS INT) AS order_size_id,\n","        items, \n","        total_amount, \n","        processing_time, \n","        IsWeather \n","    FROM table_changes('tbl_bronze_raw', {last_version}) tc\n","    WHERE _change_type = 'insert'\n","    AND total_amount >= 0\n","    \"\"\")\n","\n","    orders_df = changes_df.drop(\"items\", \"IsWeather\")\n","\n","    \n","    # extract fields list from struct<â€¦> items.\n","    item_struct = changes_df.schema[\"items\"].dataType\n","    item_names  = [f.name for f in item_struct.fields]\n","\n","    # build the list of columns for SELECT\n","    select_cols = [col(\"order_id\")]  # order_id everywhere\n","    for name in item_names:\n","        select_cols.append(\n","            col(f\"items.{name}\").alias(f\"{name}_qty\")\n","        )\n","\n","    # create generic items_df\n","    items_df = changes_df.select(*select_cols, col(\"partition_date\"))\n","\n","\n","    weather_struct = changes_df.schema[\"IsWeather\"].dataType\n","    weather_names  = [f.name for f in weather_struct.fields]\n","\n","    select_cols = [col(\"order_id\")]  # order_id everywhere\n","\n","    for name in weather_names:\n","        select_cols.append(\n","            col(f\"IsWeather.{name}\").alias(name)\n","        )\n","    weather_df = changes_df.select(*select_cols, col(\"partition_date\"))    \n","\n","    batch_id = int(current_version)\n","\n","    # add batch_id to all df\n","    orders_df  = orders_df.withColumn(\"batch_id\",  lit(batch_id))\n","    items_df   = items_df.withColumn(\"batch_id\",   lit(batch_id))\n","    weather_df = weather_df.withColumn(\"batch_id\", lit(batch_id))\n","\n","    written = []  # track of table names that have already been written\n","\n","    try:\n","        orders_df.write \\\n","            .format(\"delta\") \\\n","            .mode(\"append\") \\\n","            .option(\"mergeSchema\", \"false\") \\\n","            .saveAsTable(\"stg_silver_orders\")\n","\n","        written.append(\"stg_silver_orders\")\n","        \n","        items_df.write \\\n","            .format(\"delta\") \\\n","            .mode(\"append\") \\\n","            .option(\"mergeSchema\", \"false\") \\\n","            .saveAsTable(\"stg_silver_items\")\n","\n","        written.append(\"stg_silver_items\")\n","\n","        weather_df.write \\\n","            .format(\"delta\") \\\n","            .mode(\"append\") \\\n","            .option(\"mergeSchema\", \"false\") \\\n","            .saveAsTable(\"stg_silver_weather\")\n","\n","        written.append(\"stg_silver_weather\")\n","\n","        spark.sql(f\"\"\"\n","        UPDATE cdc_control\n","        SET last_processed_version = {current_version}\n","        WHERE name = 'nb_etl_silver'\n","        \"\"\")\n","\n","    except Exception as e:\n","        print(f\"[ERROR] Batch {batch_id} failed, rolling back: {e}\")\n","        # selective rollback only for the tables that were successfully saved\n","        for t in written:\n","            spark.sql(f\"DELETE FROM {t} WHERE batch_id = {batch_id}\")\n","        raise\n","\n","else:\n","    print(\"No new changes to process.\")    "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"7f2df7cd-5ee7-4393-a651-dfca05d9676b","normalized_state":"finished","queued_time":"2025-08-19T07:16:45.5969092Z","session_start_time":null,"execution_start_time":"2025-08-19T07:16:45.5981527Z","execution_finish_time":"2025-08-19T07:17:43.4991043Z","parent_msg_id":"1757759c-1c87-4ddb-a2d7-520b5d7a5626"},"text/plain":"StatementMeta(, 7f2df7cd-5ee7-4393-a651-dfca05d9676b, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"27422829-083b-4191-9b03-abcdc95b39d5"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"94330eaa-c205-4d05-bef4-929359e39213","known_lakehouses":[{"id":"94330eaa-c205-4d05-bef4-929359e39213"}],"default_lakehouse_name":"lh_toa","default_lakehouse_workspace_id":"bed316b0-e26c-4a89-a790-a007cce0e3fd"}}},"nbformat":4,"nbformat_minor":5}